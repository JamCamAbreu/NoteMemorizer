~ Topic 5: Connect to and consume Azure services and third-party services (15% to 20%)

	# You should be able to...IMPLEMENT API MANAGEMENT
		* Create an Azure API Management instance
		* Create and ^document APIs
		* Configure access to APIs
		* Implement ^policies for APIs

	# You should be able to...DEVELOP EVENT-BASED SOLUTIONS
		* Implement solutions that use Azure Event ^Grid
		* Implement solutions that use Azure Event ^Hub

	# You should be able to...DEVELOP MESSAGE-BASED SOLUTIONS
		* Implement solutions that use Azure ^Service ^Bus
		* Implement solutions that use Azure ^Queue ^Storage queues
		
		
		
	# Event Grid - Events
		* Smallest amount of information describing something that happened in the system
		* There is information common to all events (source, date and time, and unique ID) as well as information unique to the event
		* The General Availability (GA) SLA allows events up to 64 KB in size. Support for events up to 1MB in size is currently in preview
		* Events over 64 KB are charged in 64 KB increments
		
	# Event Grid - Event Sources
		* Where the event happened
		* Responsible for sending (i.e. publishing) events to Event Grid
		
	# Event Grid - Topics
		* Provides the endpoints where an event source sends events
		* Used for a collection of related events
		* A source can specify one or more topics
		* System topics are built-in topics provided by Azure services
			- You cannot see these in your Azure subscription, but you can subscribe to them by providing information about the resource you want to recieve events from (e.g. Blob Storage)
		* Custom topics are application and third-party topics
		* Subscribers decide which topics to subscribe to in order to respond to certain types of events
		
	# Event Grid - Subscriptions
		* Tell Event Grid which events on a topic you're interested in receiving
		* As a subscriber you provide the endpoint that will recieve (i.e. handle) the events
		* Optional filters can be supplied to filter events by event type or subject pattern
		* Subscriptions can be created with expiration date
		
	# Event Grid - Event Handlers
		* The destination to where an event is sent
		* Takes further action to process the events it receives
		* There are different types of event handlers including supported Azure services (?) or custom webhooks
		* Event Grid has different logic in place to guarantee the delivery of an event for the different types of handlers
		
	# Event Grid - Event Schemas
		* 2 Types: Event Grid event schema and Cloud event schema
		* All events consists of 4 required string properties: subject, eventType, eventTime, and id
		* The data object has properties specific to the publisher
		* Event sources sent events to Event Grid in an array consisting of one or more event objects
			* Each object can be up to 1MB in size, but the total size of the array itself cannot exceed 1 MB
		* 
		
	# Event Grid - Subject Tips
		* Consider providing the path for where events happened so subscribers can filter and route events by segments of the path
		* For example if you have a 3-segment path /A/B/C a user can choose to filter by any part of the path depending on whether they want the broad set of events (filtering on /A) or a narrower set of events (filtering on /A/B/C) or something in between (filtering on /A/B)
		
	# Event Grid - Cloud Event Schema
		* CloudEvents is an open specification for describing event data
		* Natively supported by Azure
		*
		
	# Event Grid - Delivery Durability
		* Event Grid attempts to deliver each event at least once for each matching subscription immediately
		* If the event the subscriber does not acknowledge receipt of an event or if there's a failure, Event Grid retries delivery accordingly to a fixed retry schedule and policy
		* By default, Event Grid delivers 1 event at a time to a subscriber via an array of consisting of just that 1 event
		* Event Grid does NOT guarantee order of event delivery
		
	# Event Grid - Retry Schedule
		* When Event Grid receives an error for an attempted event delivery it chooses one of the following options based on the type of error:
			- Retry the delivery
			- Dead-letter the event (optional configuration)
			- Drop the event
			
		* After 30 seconds from point of attempted delivery, if an endpoint hasn't responded, the message is queued for retry
		* If the endpoint responds within 3 minutes, Event Grid attempts to remove the event from the retry queue on a best effort basis (i.e. you might get duplicates)
		* The following endpoint errors will not produce retry attempts by Event Grid:
			- Azure Resources: 400 Bad Request, 413 Request Entity Too Large, 403 Forbidden
			- Webhook: 400, 413, 403, 404 Not Found, 401 Unauthorized
			
	# Event Grid - Retry Policy
		* You can customize the retry policy, but only when creating the subscription
			- Maximum # of Attempts: A value between 1 and 30. Default is 30.
			- Event TTL (time to live): A value between 1 and 1440. Default is 1440.
			
	# Event Grid - Output Batching
		* You can configure Event Grid to batch events for delivery to improve HTTP performance
		* Batching is turned off by default
		* 2 Settings:
			- Max Events / Batch - A value between 1 and 5000
			- Preferred Batch Size in KB - Batches that exceed this configured size will still be delivered
			
	# Event Grid - Delayed Delivery
		* As an endpoint experience delivery failures, Event Grid will begin to delay the delivery and retry of events to the endpoint
		* The purpose is to protect unhealthy endpoints and the Event Grid system itself by reducing unnecessary retries that could potentially overwhelm systems
		
	# Event Grid - Dead Letter Events
		* AN optional process where Event Grid sends undeliverable events to a storage account
		* EventGrid dead-letters an event when either of the following conditions are met:
			- Event isn't delivered within the time-to-live period
			- The number of retries exceeds the limit
		* Dead lettering is off by default
		* Configurable at the subscription creation time. The storage account location is where you can pull the undelivered events to resolve the delivery
		* There is a 5 minute delay between the last attempt to deliver and event and the delivery of that event to the dead letter location
		* In the event the dead letter location is unavailable for 4 hours, the event is dropped
		
	# Event Grid - Custom Delivery Properties
		* Event subscriptions allow HTTP headers that are included in delivered events
		* Useful for when a destination requires custom headers
		* Can set up to 10 custom headers when creating the event subscription with each header not exceeding 4,096 bytes
		* Can set the custom headers on events delivered to the following destinations:
			* Webhooks
			* Azure Service Bus Topics
			* Azure Event Hubs
			* Relay Hybrid Connections
			
	# Event Grid - Access Controls:
		* Role																	Description
		* Event Grid Subscription Reader 				Lets you read Event Grid event subscriptions
		* Event Grid Subscription Contributor		Lets you manage Event Grid event subscription operations
		* Event Grid Contributor								Lets you create and manage Event Grid resources
		* Event Grid Data Sender								Lets you send events to Event Grid topics
		
		* If you are using an event handler that isn't a webhook, you need write access to that resource. The permission will differ based on whether the subscription is for a system or custom topic
	
	
	# Event Grid - Using Webhooks
		* Event Grid requires you to prove ownership of your Webhook endpoint before it will start delivering ends to that endpoint
		* This validation is handled automatically when using the following 3 Azure Services:
			* Azure Logic Apps with Event Grid Connector
			* Azure Automation via Webhook
			* Azure Functions with Event Grid Trigger
			
		* 2 Ways to validate a subscription
			* Synchronous Handshake: Happens at the time of event subscription creation. An event is sent from EventGrid to your endpoint that includes a validationCode property that your app must verify the request by responding with the same validationCode. Newer versions also support a manual validation method using a validationUrl and GET request. The URL is valid for 5 minutes.
			
			* Asynchronous Handshake: Specifically for cases where you can't erturn the ValidationCode using the synchronous handshake. This is often the case when using third-party services (e.g. Zapier)
			
			
	# Event Gird - Filtering
		* 3 Filtering Options
			* Event Types
			* Subject "Begins With" or "End With"
			* Advanced Fields and Operators
				* Operator Type - the type of comparison
				* Key - the field (number, Boolean, or string) in the vent data you're using for filtering
				* Value(s) - the value(s) to compare to the key
				
				
	# Azure Event Hub
		* Big data streaming platform and event ingestion service
		* Lives between event publishers and event consumers
		
	# Event Hubs - Key Features:
		* Fully managed PaaS solution: Essentially Apache Kafka under the hood, but the clusters are managed by Microsoft
		* Real-time and batch processing - allows concurrent processing of the event stream as well as allows you to control the speed of the event processing
		* Capture event data - Capture data to Azure Blob Storage or Azure data lake storage
		* Scalable
		* Rich ecosystem - any Apache Kafka compatible clients can talk to Event Hubs
		
	# Event Hubs - Key Concepts
		* Event Hubs Client - The interface for developers to interact with the Event Hubs client library
		* Event Hubs Producer - a type of client that is a source/publisher of log data
		* Event Hubs Consumer - A type of client that is the consumer of log data. Often have built-in analytics (e.g. Microsoft Stream Analytics)
		* Partitions - an ordered sequence of events stored in an Event Hub. A way to provide parallel consumption of events. Number of partitions in an Event Hub specified at creation time.
		* Consumer Group - a veiw of an entire Event Hubs. Specific to each client consumer of the event stream.
		* Event Recievers - 
		TODO
		
	# Event Hubs - Capture
		* Specify whether the data is stored in an Azure Blob Storage account and container or an Azure Data Lake Store account
		* Captured data is written in the Apache Avro binary format (compatible with Hadoop, Stream Analytics, and Azure Data Factory)
		* You specify a retention period so over time and older data is automatically removed and Event Hubs remains at a manageable size
		* You set up a minimum size and time (i.e., window) with a "first wins policy"
		* Each partition captures independently to the specified destination in the following format:
			{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
			
			*Note: this means that one message could end up in multiple partitions
			
	# Event Hubs - Throughput Units
		* Event Hubs traffic controlled by throughput units
		* A single throughput unit allows 1 MB/second or 1000 events per second of ingress and twice that for egress
		* Standard SKU allows 1~20 throughput units which a quota increase via a support request
		* Usage beyond your purchase limit IS throttled by Azure
		* Egress only applies when utilizing other processing readers such as Stream Analytics or Spark
		* Once configured, Event Hubs runs automatically when you send your first event and continues to run
		* Even if there is no incoming data, Event Hubs still writes empty files to let you know it's still working as expected
		
	# Ingress vs Egress
		* Ingress: messages coming into the system
		* Egress: messages being sent out of the system
		
	# Event Hubs - Scale Process Application
		* To scale your event processing application, you can run multiple instances of the application and have it balance the load among themselves
		* For the .NET and Java SDK use EventProcessingClient
		* For Python and Javascript use EventHubConsumerClient
		* An event processor instance typically owns and processes events from one or more partitions
		* Ownership of partitions is evenly distributed among all the active event processor instances associated with an event hub and consumer group combination
		* Each event processor is given a unique identifier and claims ownership of partitions by adding or updating an entry in a checkpoint store
		
	# Event Hubs - Checkpointing
		* Checkpointing is a process by which an event processor marks or commits the position of the last successfully processed event within a partition
		* Marking a checkpoint is typically done within the function that processes the events and occurs on a per-partition basis within a consumer group
		* You can use checkpointing to both mark events as "complete" by downstream applications and to provide resiliency when an event processor goes down
		
	# Event Hubs - Receive Messages
		* When you create an event processor, you specify the functions that process events and errors
		* Each call to the function that processes events delivers a single event from a specific partition.
		* It's your responsibility to handle this event
		* If you want to make sure the consumer processes every message at least once, you need to write your own code with retry logic
		* The function that processes the events is called sequentially for a given partition meaning subsequent events and calls to the function are queued up
		* Events from different partitions can be processed concurrently and any shared state that is accessed across partitions must be synchronized
		
	# Event Hubs - Event Access
		* Event Hubs supports both Microsoft Entra ID and shared access signatures (SAS) for authentication and authorization
		* Built-in roles for controlling access include Azure Event Hubs Data Owner, Azure Event Hubs Data Sender, and Azure Event Hubs Data Receiver
		* Microsoft Entra
			* Includes support for managed identities
			* No need to store credentials in code
			* Use a returned OAuth 2.0 token instead
			
	# Event Hubs - Event Access Continued
		* Shared Access Signatures
			* An event publisher defines a virtual endpoint for Event Hubs
			* Publishers enable fine-grained access control
			* Event Hubs clients are assigned a unique token uploaded to the client
			* A client with a token can only send to 1 publisher
			* However, multiple clients can share the same token (and thus publisher)
			* All tokens are assigned with SAS keys
			* Clients are not aware of the key and the client can use the key until it expires
			* To authenticate back-end applications that consume from the data generated by Event Hubs producers, Event Hubs token authentication requires its clients to either have the manage rights or the listen privileges assigned to its Event Hubs namespace or event hub instance or topic
			* Data is consumed from Event Hubs using consumer groups
			
		# Publishing Events
			* Producers publish events in batches and may request a specific partition, or allow the Event Hubs service to decide which partition events should be published to.
			* When the publishing of events needs to be highly available or when event data should be distributed evenly among the partitions automatic routing is recommended.
			
		# Processing Events Hubs Events
			* For most production scenarios, it's recommended that the EventProcessorClient be used for reading and processing events
			* EventProcessorClient provides a more robust and performant experience
			* EventProcessClient has a depdendency on Azure Storage blobs for persistence of its state, 
				-- CHECK SLIDE HERE
				